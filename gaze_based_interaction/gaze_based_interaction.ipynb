{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad744890-9780-427d-bae6-e5a9adfbdff8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gaze-based interaction\n",
    "\n",
    "Andrew Howes\n",
    "\n",
    "June 2022\n",
    "\n",
    "Gaze-based interaction is the problem of selecting items presented on a computer with eye movements and fixations. The problem for the computer is to determine when a user's fixation represents the intention to select. The problem for the user is to make efficient eye movements towards an intended target and to hold fixation on the target.\n",
    "\n",
    "Gaze-based interaction has been studied in human-computer interaction (HCI) since the 1990s and interest has been maintained throughout the intervening years. Recent technological advances, such as the addition of eye tracking capabilities to virtual reality and augmented reality (AR), have sparked another wave of interest.\n",
    "\n",
    "One reason that gaze-based interaction is interesting is that eye-movements are highly strategic. For example, eye-movements tend to undershoot a target, presumably because, in the presence of uncertainty, undershooting costs less time than overshooting. Another interesting property of eye-movements to a target is that they obey Fitts's Law; that is the movement time of the eyes is proportional to the distance and inversely proportional to the size of the target.\n",
    "\n",
    "The purpose of this notebook is to introduce a reinforcement learning model of how human's select a target by moving their eyes to it. The model is a version of that presented in Chen et al. (2021) that has been restructured for tutorial purposes.\n",
    "\n",
    "Rather than strategies being programmed into the model, instead a policy for how to move the eyes is learned using reinforcement learning. The learned policy is the optimal solution to a problem that is defined by the constraints imposed by the oculomotor and perceptual systems. These constraints include the fact that eye-movements, fixations and visual perception are stochastic. Eye-movements for example, do not go exactly where cognition intends.\n",
    "\n",
    "The constraints are defined in a cognitive architecture -- an information processing model of the mind.\n",
    "\n",
    "### The cognitive architecture\n",
    "\n",
    "<img src=\"image/architecture.png\" alt=\"Box diagram of a cognitive model.\" width=\"700\" height=\"400\">\n",
    "\n",
    "A cognitive architecture represents a commitment to a theory of how information is processed in the human mind. The cognitive architecture in the figure consists of five modules (inside the dashed box) that interact with an external world.\n",
    "\n",
    "Below, the modules have the following properties:\n",
    "\n",
    "* **World** On each trial, a circular target with a diameter \"width\" and a location (x,y) appears in the World. Target locations are randomly sampled from a bivariate Gaussian distribution centred at (0,0).\n",
    "* **Perception** The location of target location stimuli are corrupted by Gaussian noise in human vision. The standard deviation of noise increases linearly with eccentricity from the fovea. \n",
    "* **Memory** Sequences of stimuli are optimally integrated.\n",
    "* **Utility** Rewards are received for getting closer to the target.\n",
    "* **Motor** Intended eye movements (oculomotor actions) are corrupted by signal dependent Gaussian noise to generate responses.\n",
    "* **Control**  The Control module observes the memory and receives a reward. It learns a policy for making eye movements to the target that maximises the reward.\n",
    "\n",
    "A python implementation of each module is described below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebf7d7-774c-4374-ba2f-a8679e7220de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Configuration and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oeJU5wJR7A8V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1652,
     "status": "ok",
     "timestamp": 1643210006123,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "oeJU5wJR7A8V",
    "outputId": "f8e53d98-820c-4a31-b9f5-e4b5c46bd72b"
   },
   "outputs": [],
   "source": [
    "# This cell only for users of Google Colab.\n",
    "# No need to run this if you are using Jupyter notebooks\n",
    "# Mount Google drive and change directory into the project folder\n",
    "# Only needs to be run once\n",
    "\n",
    "#!pip install stable_baselines3\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#%cd '/content/drive/MyDrive/CHI22CMT/CHI22_CogMod_Tutorial/03-Reinforcement-Learning/034_Gaze_based_Interaction'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "P44tljjb8hBC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6305,
     "status": "ok",
     "timestamp": 1643209694066,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "P44tljjb8hBC",
    "outputId": "d635fb70-1df0-4ab3-f15e-13b3d7b2fdc9"
   },
   "outputs": [],
   "source": [
    "# Load local modules\n",
    "# gazetools is a module that contains functions for modeling gaze-based interaction.\n",
    "\n",
    "from gazetools import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554f211-129f-499d-942a-8ba18735be9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-missile",
   "metadata": {
    "id": "boolean-missile",
    "tags": []
   },
   "source": [
    "### World Module\n",
    "\n",
    "The World module allows us to make predictions from the theory for a particular task. The theory makes predictions for many more tasks. For example, adaptation to mixed target widths and distances. The World generates a **stimulus** that includes the target location and width and the agent can **respond** with an eye-movement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacterial-archives",
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1643209742334,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "bacterial-archives"
   },
   "outputs": [],
   "source": [
    "class GazeTask():\n",
    "    \n",
    "    def __init__(self, theta):\n",
    "        self.theta=theta\n",
    "\n",
    "    def reset(self):\n",
    "        ''' The external_state includes the fixation and target location.\n",
    "        Choose a new target location and reset to the first fixation location.'''\n",
    "        \n",
    "        def _get_new_target():\n",
    "            x_target = np.clip(np.random.normal(0, self.theta['target_loc_std']),-1,1)\n",
    "            y_target = np.clip(np.random.normal(0, self.theta['target_loc_std']),-1,1)         \n",
    "            return np.array( [x_target, y_target] )\n",
    "    \n",
    "        # the initial fixation\n",
    "        self.fixation = np.array([-1,-1])\n",
    "        # the target location for this episode\n",
    "        self.target = _get_new_target()\n",
    "\n",
    "    def step(self, response):\n",
    "        self.fixation = response\n",
    "        \n",
    "        # determine when the goal has been achieved.\n",
    "        distance = get_distance(self.fixation, self.target)\n",
    "        if distance < self.theta['target_width']/2 :\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        stimulus = {'fixation': self.fixation, 'target':self.target, 'width':self.theta['target_width']}\n",
    "        return stimulus, done\n",
    "\n",
    "\n",
    "class World(GazeTask):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ead0e8-481f-4af5-8351-433684c306da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perception Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9b29d5-2ab8-4f65-9bab-802d60507b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vision():\n",
    "\n",
    "    def __init__(self, theta):\n",
    "        self.theta = theta\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, stimulus):\n",
    "        ''' define a psychologically plausible visual perception function in which acuity \n",
    "        falls off with eccentricity.''' \n",
    "        eccentricity = get_distance( stimulus['target'], stimulus['fixation'] )\n",
    "        estimate_std = self.theta['perceptual_noise_weight'] * eccentricity\n",
    "        noise = np.random.normal(0, estimate_std, stimulus['target'].shape)\n",
    "        # stimulus is the external target location plus noise\n",
    "        estimate = stimulus['target'] + noise\n",
    "        percept = {'value': estimate, 'noise':estimate_std, 'distance':eccentricity, 'width':stimulus['width']}\n",
    "        return percept\n",
    "\n",
    "class Perception(Vision):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31c382-0988-4cd9-bf9c-8f629cd8b07a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a7f30f-167d-4080-bcfe-de9db2129635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    ''' Keep a memory of percepts. Compute an observation of memory. '''\n",
    "    \n",
    "    def __init__(self, theta):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.belief = np.array([0,0])\n",
    "        self.belief_std = 0.1\n",
    "        observation = np.array([self.belief[0], self.belief[1], self.belief_std])\n",
    "        return observation\n",
    "    \n",
    "    def step(self, percept):\n",
    "        posterior, posterior_std = self.bayes_update(percept['value'], \n",
    "                                                     percept['noise'], \n",
    "                                                     self.belief,\n",
    "                                                     self.belief_std)\n",
    "        self.belief = posterior\n",
    "        self.belief_std = posterior_std\n",
    "\n",
    "        observation = np.array([self.belief[0], self.belief[1], self.belief_std])\n",
    "        return observation\n",
    "\n",
    "    def bayes_update(self, percept, percept_std, prior, prior_std):\n",
    "        ''' A Bayes optimal function that integrates multiple stimuli.\n",
    "        The belief is the prior.'''\n",
    "        z1, sigma1 = percept, percept_std\n",
    "        z2, sigma2 = prior, prior_std\n",
    "        w1 = sigma2**2 / (sigma1**2 + sigma2**2)\n",
    "        w2 = sigma1**2 / (sigma1**2 + sigma2**2)\n",
    "        posterior = w1*z1 + w2*z2\n",
    "        posterior_std = np.sqrt( (sigma1**2 * sigma2**2)/(sigma1**2 + sigma2**2) )\n",
    "        return posterior, posterior_std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed508a-784e-4ef5-9333-4c52a39fb0a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d516283-c998-4af5-8757-b07836053f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Utility():\n",
    "    ''' Compute the reward given an observation '''\n",
    "    \n",
    "    def __init__(self, theta):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, observation):\n",
    "        \n",
    "        if observation['distance'] < observation['width'] / 2:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -observation['distance'] # a much better model of the psychological reward function is possible.\n",
    "            \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44745453-faf5-4b53-bf4d-ad80b8f19c79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Motor Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aea2b1b-0778-46b1-9448-d21240c5dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Oculomotor():\n",
    "    \n",
    "    def __init__(self, theta):\n",
    "        ''' The current fixation is part of the motor system state. '''\n",
    "        self.theta = theta\n",
    "        self.fixation = np.array([-1,-1])\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''  Add noise to the action and execute it in the world.'''\n",
    "        distance = get_distance( self.fixation, action )\n",
    "        \n",
    "        oculomotor_noise = np.random.normal(0, self.theta['oculomotor_noise_weight'] * distance, action.shape)\n",
    "        \n",
    "        # response is action plus noise\n",
    "        response = action + oculomotor_noise\n",
    "        \n",
    "        # update the oculomotor state\n",
    "        self.fixation = response\n",
    "        \n",
    "        # make an adjustment if response is out of range. \n",
    "        response = np.clip(response,-1,1)\n",
    "        \n",
    "        return response\n",
    "\n",
    "class Motor(Oculomotor):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-alliance",
   "metadata": {
    "id": "combined-alliance",
    "tags": []
   },
   "source": [
    "### Control Module\n",
    "\n",
    "The control module uses RL to find an optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157df96a-cad2-4fbb-9f2c-eda56738044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Control():\n",
    "    \n",
    "    def __init__(self, theta, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def train(self, timesteps):\n",
    "        ''' Train the control module to find the optimal policy for the Gym environment.\n",
    "            Monitor() and PPO() are part of the stable_baselines3 package.\n",
    "            PPO (Proximal Policy Optimisation) is a class of RL algorithm https://openai.com/blog/openai-baselines-ppo/\n",
    "            '''\n",
    "        self.env = Monitor(self.env, output_dir)\n",
    "        self.policy = PPO('MlpPolicy', self.env, verbose=0, clip_range=0.15)\n",
    "        self.policy.learn(total_timesteps=int(timesteps))\n",
    "        self.policy.save(f'{output_dir}{policy_file}')\n",
    "        print('Done training.')\n",
    "\n",
    "    def run(self, n_episodes, filename):\n",
    "        '''\n",
    "        run the model for n_episodes and save its behaviour in a csv file.\n",
    "        Note that 'env' is a term used by Gym to describe everything but the controller.\n",
    "        '''\n",
    "        max_episodes = 900000\n",
    "        if n_episodes > max_episodes:\n",
    "            print(f'We ask that you limit training to a max of {max_episodes} on the School of Computer Science AWS account.')\n",
    "            print(f'If you want to run more training episodes then please do so on a local computer.')\n",
    "            return\n",
    "\n",
    "        result = []\n",
    "        # repeat for n episodes\n",
    "        eps = 0\n",
    "        while eps < n_episodes:                \n",
    "            done = False\n",
    "            step = 0\n",
    "            obs = self.env.reset()\n",
    "            # record the initial state\n",
    "            info = self.env.get_info(obs, 0, False)\n",
    "            info['episode'] = eps\n",
    "            result.append(info)\n",
    "            # repeat until the gaze is on the target.\n",
    "            while not done:\n",
    "                step+=1\n",
    "                # get the next prediction action from the policy\n",
    "                action, _ = self.policy.predict(obs,deterministic = True)\n",
    "                obs, reward, done, info = self.env.step(action)\n",
    "                info['episode'] = eps\n",
    "                result.append( info )\n",
    "                if done:\n",
    "                    eps+=1\n",
    "        path = f'{output_dir}{filename}'\n",
    "        df = pd.DataFrame(result)\n",
    "        df.to_csv(path,index=False)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957556de-856e-4399-9cf5-b7d12f7db751",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Model\n",
    "\n",
    "The model is our model of cognition and the world. It is the environment for the control module. It defines the information flow between the modules defined above. \n",
    "\n",
    "It is defined as a Gym environment. For further information see: https://gym.openai.com/. gym.Env is a class provided by this library. The environments step function receives the action the controller and returns an observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6014653-7d44-4cc7-bbc3-cc53df9ec9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CogEnv(gym.Env):\n",
    "    ''' The CogEnv defines the environment for the controller.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, theta={}, max_steps=10):\n",
    "\n",
    "        self.theta = theta\n",
    "        \n",
    "        self.motor = Motor(theta)\n",
    "        self.world = World(theta)\n",
    "        self.perception = Perception(theta)\n",
    "        self.utility = Utility(theta)\n",
    "        self.memory = Memory(theta)\n",
    "        # The CogEnv defines the environment for the controller and therefore a Control object is not defined here.\n",
    "        \n",
    "        # The action_space and observation_space are required by gym. These define the range of each variable.\n",
    "        # Each action has an x,y coordinate therefore the box size is 2.\n",
    "        # Each obs has a an x,y and an uncertainty therefore the box size is 3.\n",
    "        \n",
    "        self.action_space = default_box(2)\n",
    "        self.observation_space = default_box(3)\n",
    "        \n",
    "        # max_fixations per episode. Used to curtail exploration early in training.\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.motor.reset()\n",
    "        self.world.reset()\n",
    "        self.perception.reset()\n",
    "        self.utility.reset()\n",
    "        observation = self.memory.reset()\n",
    "\n",
    "        self.n_step = 0\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' Define the information flow between the modules. '''\n",
    "        response = self.motor.step(action)\n",
    "        stimulus, done = self.world.step(response)\n",
    "        percept = self.perception.step(stimulus)\n",
    "        observation = self.memory.step(percept)\n",
    "        reward = self.utility.step(percept)\n",
    "        \n",
    "        self.n_step+=1\n",
    "        # give up if been looking for too long\n",
    "        if self.n_step > self.max_steps:\n",
    "            done = True\n",
    "            reward = -2\n",
    "            \n",
    "        info = self.get_info( observation, reward, done )\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def get_info(self, observation, reward, done ):\n",
    "        return {'step': self.n_step,\n",
    "                #'observation': observation,\n",
    "                'reward': reward,\n",
    "                'done': done,\n",
    "                'target_width': self.theta['target_width'],\n",
    "                'target_x': self.world.target[0],\n",
    "                'target_y': self.world.target[1],\n",
    "                'fixation_x': self.world.fixation[0],\n",
    "                'fixation_y': self.world.fixation[1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daaacb8-115a-4e8c-a316-a189cdbadae5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d53ed-a160-47ff-9432-3339f07c36ca",
   "metadata": {},
   "source": [
    "Define the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0260eb-313c-4082-b92e-6a21cd2120eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {'perceptual_noise_weight': 0.09,\n",
    "         'target_width': 0.15,\n",
    "         'target_loc_std': 0.3,\n",
    "         'oculomotor_noise_weight': 0.01\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6866d-fd75-43ee-b359-fef3a1870317",
   "metadata": {
    "id": "complex-covering",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34811351-d24c-4dd8-8269-ab2a31fae489",
   "metadata": {},
   "source": [
    "Step through the untrained model to check for simple bugs. Output a DataFrame with a row for each step.\n",
    "More comprehensive tests needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37df9347-c1d2-4425-b971-a2d01bad0025",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1643209756680,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "4ed00048",
    "outputId": "95f13fd2-30ad-43d9-8cab-1d127f6359d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>reward</th>\n",
       "      <th>done</th>\n",
       "      <th>target_width</th>\n",
       "      <th>target_x</th>\n",
       "      <th>target_y</th>\n",
       "      <th>fixation_x</th>\n",
       "      <th>fixation_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.866303</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>0.345225</td>\n",
       "      <td>0.460063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.767400</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.067748</td>\n",
       "      <td>0.584841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.047992</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>0.780278</td>\n",
       "      <td>-0.051375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.395106</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.392773</td>\n",
       "      <td>0.215472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.727718</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>0.465320</td>\n",
       "      <td>-0.147364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.359554</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.542968</td>\n",
       "      <td>-0.382248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.950125</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.071345</td>\n",
       "      <td>0.773252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.455036</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.520690</td>\n",
       "      <td>-0.532057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.443442</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.341437</td>\n",
       "      <td>0.278848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.145786</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>-0.745485</td>\n",
       "      <td>0.881454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.15748</td>\n",
       "      <td>0.225165</td>\n",
       "      <td>0.122484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    step    reward   done  target_width  target_x  target_y  fixation_x  \\\n",
       "0      1 -0.866303  False          0.15 -0.262329  -0.15748    0.345225   \n",
       "1      2 -0.767400  False          0.15 -0.262329  -0.15748   -0.067748   \n",
       "2      3 -1.047992  False          0.15 -0.262329  -0.15748    0.780278   \n",
       "3      4 -0.395106  False          0.15 -0.262329  -0.15748   -0.392773   \n",
       "4      5 -0.727718  False          0.15 -0.262329  -0.15748    0.465320   \n",
       "5      6 -0.359554  False          0.15 -0.262329  -0.15748   -0.542968   \n",
       "6      7 -0.950125  False          0.15 -0.262329  -0.15748   -0.071345   \n",
       "7      8 -0.455036  False          0.15 -0.262329  -0.15748   -0.520690   \n",
       "8      9 -0.443442  False          0.15 -0.262329  -0.15748   -0.341437   \n",
       "9     10 -1.145786  False          0.15 -0.262329  -0.15748   -0.745485   \n",
       "10    11 -2.000000   True          0.15 -0.262329  -0.15748    0.225165   \n",
       "\n",
       "    fixation_y  \n",
       "0     0.460063  \n",
       "1     0.584841  \n",
       "2    -0.051375  \n",
       "3     0.215472  \n",
       "4    -0.147364  \n",
       "5    -0.382248  \n",
       "6     0.773252  \n",
       "7    -0.532057  \n",
       "8     0.278848  \n",
       "9     0.881454  \n",
       "10    0.122484  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CogEnv( theta=theta, max_steps=10 )\n",
    "\n",
    "model.reset()\n",
    "\n",
    "def step_test():\n",
    "    j=0\n",
    "    result = []\n",
    "    done = False\n",
    "    model.reset()\n",
    "    while not done:\n",
    "        # make a step with a randomly sampled action\n",
    "        obs, reward, done, info = model.step(model.action_space.sample())\n",
    "        result.append(info)\n",
    "        j+=1\n",
    "    df = pd.DataFrame(result)\n",
    "    return df, j\n",
    "\n",
    "info, i = step_test()\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a8d46-a29c-4aad-9b5d-026924ef86c3",
   "metadata": {},
   "source": [
    "Calculate the average number of steps taken by the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a8a530d-b199-434f-8d41-7f047cf81302",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1643209756680,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "4ed00048",
    "outputId": "95f13fd2-30ad-43d9-8cab-1d127f6359d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202.0\n"
     ]
    }
   ],
   "source": [
    "def mean_steps_taken():\n",
    "    rs = []\n",
    "    for k in range(0,10):\n",
    "        info, i = step_test()\n",
    "        rs.append(i)\n",
    "    return sum(rs) / len(rs)\n",
    "\n",
    "model.max_steps = 1000\n",
    "\n",
    "r = mean_steps_taken()\n",
    "        \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-stevens",
   "metadata": {
    "id": "cognitive-stevens",
    "tags": []
   },
   "source": [
    "## Train the control module\n",
    "\n",
    "We can train the control module to generate a policy.\n",
    "\n",
    "By plotting the learning curve we can see whether the performance improves with training and whether the model approaches an optimum performance. We are interested in approximately optimal performance, so if the training curve is not approaching asymptote then we need to train with more timesteps or revise the model.\n",
    "\n",
    "We can see that at first the model uses hundreds of fixations to find the target, this is because it has not yet learned to move the gaze in a way that is informed by the observation. As it learns to do this, it takes fewer steps to gaze at the target and its performance improves.\n",
    "\n",
    "If our problem definition is correct then the model will get more 'human-like' the more that it is trained. In other words, training makes it a better model of interaction.\n",
    "\n",
    "If we assume that people are computationally rational then the optimal solution to the gaze problem (as defined above) predicts human behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0910ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 66705,
     "status": "ok",
     "timestamp": 1643209827031,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "7d0910ac",
    "outputId": "d927873a-e745-4931-d6b8-fb6f3a668245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.max_steps = 20\n",
    "control = Control(theta, model)\n",
    "\n",
    "timesteps = 300000\n",
    "\n",
    "control.train(timesteps)\n",
    "plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2797581-e41d-4859-9c7c-aaafa98dd00f",
   "metadata": {},
   "source": [
    "You should see a 'learning curve' in which as the model is trained with more 'timesteps' its performance improves. With more timesteps the rewards of each episode increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-georgia",
   "metadata": {
    "id": "japanese-georgia",
    "tags": []
   },
   "source": [
    "### Run the model\n",
    "Run the trained model and save a trace of each episode to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "after-evans",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 162308,
     "status": "ok",
     "timestamp": 1643210003598,
     "user": {
      "displayName": "Andrew Howes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguyjUymXH2ndqd0p8hhQuI6UyIwWtm4lsMYWs0Ug=s64",
      "userId": "02694399383679444060"
     },
     "user_tz": 0
    },
    "id": "after-evans",
    "outputId": "838f8f2b-9414-454a-8700-acfd39062bb8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>reward</th>\n",
       "      <th>done</th>\n",
       "      <th>target_width</th>\n",
       "      <th>target_x</th>\n",
       "      <th>target_y</th>\n",
       "      <th>fixation_x</th>\n",
       "      <th>fixation_y</th>\n",
       "      <th>episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.154578</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.204402</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.154578</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>-0.041435</td>\n",
       "      <td>-0.021915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.154578</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>0.165105</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.408209</td>\n",
       "      <td>-0.529194</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.679247</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.408209</td>\n",
       "      <td>-0.529194</td>\n",
       "      <td>-0.042591</td>\n",
       "      <td>-0.021103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.215824</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.169641</td>\n",
       "      <td>-0.073572</td>\n",
       "      <td>-0.038866</td>\n",
       "      <td>-0.017851</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.169641</td>\n",
       "      <td>-0.073572</td>\n",
       "      <td>0.180348</td>\n",
       "      <td>-0.095328</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.071729</td>\n",
       "      <td>-0.289172</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.287737</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.071729</td>\n",
       "      <td>-0.289172</td>\n",
       "      <td>-0.042456</td>\n",
       "      <td>-0.025061</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.071729</td>\n",
       "      <td>-0.289172</td>\n",
       "      <td>0.119618</td>\n",
       "      <td>-0.297592</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3255 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      step    reward   done  target_width  target_x  target_y  fixation_x  \\\n",
       "0        0  0.000000  False          0.15  0.154578  0.036043   -1.000000   \n",
       "1        1 -0.204402  False          0.15  0.154578  0.036043   -0.041435   \n",
       "2        2  0.000000   True          0.15  0.154578  0.036043    0.165105   \n",
       "3        0  0.000000  False          0.15  0.408209 -0.529194   -1.000000   \n",
       "4        1 -0.679247  False          0.15  0.408209 -0.529194   -0.042591   \n",
       "...    ...       ...    ...           ...       ...       ...         ...   \n",
       "3250     1 -0.215824  False          0.15  0.169641 -0.073572   -0.038866   \n",
       "3251     2  0.000000   True          0.15  0.169641 -0.073572    0.180348   \n",
       "3252     0  0.000000  False          0.15  0.071729 -0.289172   -1.000000   \n",
       "3253     1 -0.287737  False          0.15  0.071729 -0.289172   -0.042456   \n",
       "3254     2  0.000000   True          0.15  0.071729 -0.289172    0.119618   \n",
       "\n",
       "      fixation_y  episode  \n",
       "0      -1.000000        0  \n",
       "1      -0.021915        0  \n",
       "2       0.031278        0  \n",
       "3      -1.000000        1  \n",
       "4      -0.021103        1  \n",
       "...          ...      ...  \n",
       "3250   -0.017851      998  \n",
       "3251   -0.095328      998  \n",
       "3252   -1.000000      999  \n",
       "3253   -0.025061      999  \n",
       "3254   -0.297592      999  \n",
       "\n",
       "[3255 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control.run( 1000, 'behaviour_trace.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdefc5-7dfb-481e-a887-729c7a08be18",
   "metadata": {},
   "source": [
    "###Excercise\n",
    "\n",
    "Retrain the control module but with timesteps=300000. Inspect the learning curve and note that there is now a clear asymptote in performance which approaches 0 loss.\n",
    "\n",
    "Rerun the model to generate a new behaviour_trace.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-capture",
   "metadata": {
    "id": "stable-capture",
    "tags": []
   },
   "source": [
    "## Next\n",
    "\n",
    "Congratulations, you have now completed a Python notebook that introduces you to how to model cognition with RL. \n",
    "\n",
    "Now go to notebook 'visualise' to see an animation of the results in behaviour_trace.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-headquarters",
   "metadata": {
    "id": "weekly-headquarters"
   },
   "source": [
    "## References\n",
    "Chen, X., Acharya, A., Oulasvirta, A., & Howes, A. (2021, May). An adaptive model of gaze-based selection. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-11)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gaze-based-interaction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
